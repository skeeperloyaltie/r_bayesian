# -*- coding: utf-8 -*-
"""R_file.R

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jMRjNAXUzMz-JKI-dilRK6EhgSsfZ9-O

# PART A BAYESIAN MODELS
"""

library(tidyverse)
library(ggplot2)
install.packages("timelineR")
library(timelineR)

polution <- read.csv("London_Pollution.csv")

# Remove blank data  in the dataset
polution <- na.omit(polution)
head(polution)
#Change data to date

polution$Date <- as.Date(polution$Date)

summary(polution)
hist(polution$Bloomsbury)

ggplot(polution, aes(x=Barking)) + geom_histogram()

require(timelineR)
polution$Date = as.POSIXct(polution$Date)
plot_timeline <- plot_timeline(polution)

# split data

trainTestSplit <- function(polution,trainPercent,seed1){

smp_size <- floor(trainPerNew Sectioncent/100 * nrow(polution))

set.seed(seed1)
train_ind <- sample(seq_len(nrow(polution)), size = smp_size)
train_ind
}
train_ind <- trainTestSplit(polution,trainPercent=85,seed=123)
train <- polution[train_ind, ]
test <- polution[-train_ind, ]
head(polution)



head(train)
tail(test)

#fit Model
model <- lm(Bloomsbury ~ Barking, data = polution)
prediction <- predict(model, newdata = polution)
plot(prediction)
summary(prediction)

jags.mod <- function(){
B.pred[1] ~ dnorm(0, 1.0E-3)
for (i in 2 : N) {
Bloomsbury[i] ~ dnorm(B.pred[i],tau.v)
B.pred[i] ~ dnorm(B.pred[i-1], tau.w)
}
# priors
tau.w ~ dgamma(1,0.01)
sigma.w2 <- 1/tau.w
tau.v ~ dgamma(1,0.01)
sigma.v2 <- 1/tau.v
}

#fit Model
model <- lm(Bloomsbury ~ Barking, data = polution)
prediction <- predict(model, newdata = polution)
plot(prediction)
summary(prediction)

"""# Classification"""

install.packages("caret")
install.packages("MASS")
install.packages('e1071')
library(e1071)
library(caret)
library(MASS)

classified <- read.csv("Classification.csv")
head(classified)

# remove nas
classified <- na.omit(classified)
summary(classified)
# Findings
# All the X1 and X2 are int variables

str(classified)

# test and train
smp_size <- floor(0.80 * nrow(classified))

set.seed(123)
train_ind <- sample(seq_len(nrow(classified)), size = smp_size)

train <- classified[train_ind, ]
test <- classified[-train_ind, ]

preproc.parameter <- train %>% 
  preProcess(method = c("center", "scale"))
  
# Transform the data using the estimated parameters
train.transform <- preproc.parameter %>% predict(train)
test.transform <- preproc.parameter %>% predict(test)
head(train.transform)

model_lda <- lda(Group ~ X2, data = train.transform)
model_lda

#Prediction 
# Quadratic discrimanant analysis
model_qda <- qda(Group ~ X2, data = train.transform)
model_qda

# Logistic regression
model <- glm(Group ~.,family=binomial(link='logit'),data=train)
model

# Support vector machines
classifier = svm(formula = Group ~ .,
                 data = train,
                 type = 'C-classification',
                 kernel = 'linear')
classifier

train_x = train[, -1]
train_x = scale(train_x)[,]
train_y = train[,2]

test_x = test[, -1]
test_x = scale(test[,-1])[,]
test_y = test[,1]

knnmodel = knnreg(train_x, train_y)
 
summary(knnmodel)

# predict
pred_y = predict(knnmodel, data.frame(test_x))
print(data.frame(test_y, pred_y))

mse = mean((test_y - pred_y)^2)
mae = caret::MAE(test_y, pred_y)
rmse = caret::RMSE(test_y, pred_y)
cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmse)

# last question
cla <- read.csv("ClassificationTrue.csv")
set.seed(123)
train_ind <- sample(seq_len(nrow(cla)), size = smp_size)

train <- cla[train_ind, ]
test <- cla[-train_ind, ]
train_x = train[, -1]
train_x = scale(train_x)[,]
train_y = train[,2]

test_x = test[, -1]
test_x = scale(test[,-1])[,]
test_y = test[,1]

knnmodel = knnreg(train_x, train_y)
 
summary(knnmodel)